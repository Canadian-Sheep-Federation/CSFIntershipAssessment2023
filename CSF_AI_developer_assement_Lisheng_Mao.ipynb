{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c433bf1e-ea8e-4ebf-8b3a-fbbfc7f1e088",
   "metadata": {},
   "source": [
    "## Assesment question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82cec7-7ecf-4a62-ad65-d23f5148fc8e",
   "metadata": {},
   "source": [
    "__Q: Given a jupyter notebook that has a functioning implementation of a machine learning model that identifies unique individuals out of a crowd through gait analysis, how would you translate that notebook to a piece of software that can be used to apply the model to any arbitrary images or videos provided.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656ceca-4e86-406a-baed-7359c8c640f5",
   "metadata": {},
   "source": [
    "__A:__ As we are dealing with images or videos data, we might probably use neural networks to identify unique individuals. In this case, I would recommend using transfer learning to apply the model to new data. Here are the steps of applying transfer learning:  \n",
    "1. Train the model on the original task.\n",
    "2. Change the format of new input data so that they can be passed to the original model.\n",
    "3. Freeze the layers of the original model but change the output layer as per our task. E.g. the original model has 10 classification outputs while the new task has 2 outputs. We need to change the 10 outputs to 2.\n",
    "4. Train and fine tune the new model on new data set. We still need to create train and valid set from the new data set.  \n",
    "\n",
    "The advantage of transfer learning is that we could train the new model on a relatively small data set but get good performance.  \n",
    "    \n",
    "For traditional tabular data, we could use joblib or pickle to save the parameters of the model and reload the model to make prediction on new data.  \n",
    "For both two methods, we need to make sure that the original task and the new task are similar so that the knowledge learned by original model can be transferred to a new context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc189f-98c7-464e-b659-06b98346b7fb",
   "metadata": {},
   "source": [
    "## Toy implementation of using CNN to classify hand written digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6006ab37-10b5-4fa9-976b-741e20188420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'font.size': 14, 'axes.labelweight': 'bold', 'axes.grid': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ce338a-ea76-49e3-8d81-60f1be5ae7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eed59b2000540c4bc0321af53ef44a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4bbb755a4544ae920f44ac3bfc402f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\train-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6c2234e7124fdb96bf39a86801b094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5856e3b8fc5b464d85b3577f1c758356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maoli\\miniconda3\\envs\\dsci572env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "# Download data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST('data/', download=True, train=True, transform=transform)\n",
    "validset = datasets.MNIST('data/', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd8bfc8-4462-43a9-b034-f7aa6a04457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN architecture\n",
    "\n",
    "class MNIST_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, (5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e171a3-208c-4887-b586-645d1d4ae6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training function\n",
    "\n",
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True):\n",
    "    \n",
    "    train_loss, valid_loss, valid_accuracy = [], [], []\n",
    "    for epoch in range(epochs):  \n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_batch_loss += loss.item()\n",
    "        train_loss.append(train_batch_loss / len(trainloader))\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for X, y in validloader:\n",
    "                y_hat = model(X)\n",
    "                _, y_hat_labels = torch.softmax(y_hat, dim=1).topk(1, dim=1)\n",
    "                loss = criterion(y_hat, y)\n",
    "                valid_batch_loss += loss.item()\n",
    "                valid_batch_acc += (y_hat_labels.squeeze() == y).type(torch.float32).mean().item()\n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b50f97e3-edff-441f-ade0-d082f56ea211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.372. Valid Loss: 0.130. Valid Accuracy: 0.96.\n",
      "Epoch 2: Train Loss: 0.115. Valid Loss: 0.074. Valid Accuracy: 0.98.\n",
      "Epoch 3: Train Loss: 0.074. Valid Loss: 0.054. Valid Accuracy: 0.98.\n",
      "Epoch 4: Train Loss: 0.060. Valid Loss: 0.044. Valid Accuracy: 0.99.\n",
      "Epoch 5: Train Loss: 0.048. Valid Loss: 0.043. Valid Accuracy: 0.99.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "model = MNIST_classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "results = trainer(model, criterion, optimizer, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a0364-e38d-43a5-9d0e-f0063c4fb5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsci572env]",
   "language": "python",
   "name": "conda-env-dsci572env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
